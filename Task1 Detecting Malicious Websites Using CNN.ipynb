{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MALICIOUS WEB DETECTION WITH 1D CNN (Convolution Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import models, layers, backend, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other setup\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "pd.set_option('max_colwidth', 500)\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('./malicious_url_dataset.csv')\n",
    "# shuffle data\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "print(f'Data size: {data.shape}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we use the holdout method for validation method which separates training and test data by 80% and 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "train_data, val_data = train_test_split(data, test_size=val_size, stratify=data['label'], random_state=0)\n",
    "print(f'Train shape: {train_data.shape}, Validation shape: {val_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Feature Engineering\n",
    "We perform some data analysis to expand our knowledge of this data and do some feature engineering. First we want to find out whether the data is imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts().plot.barh()\n",
    "plt.title('All Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good, bad = data.label.value_counts()\n",
    "print(f'Ratio of data between target labels (bad & good) is {bad//bad}:{good//bad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets find out the most used suffix domain, domain and sub domain. We need to extract subdomains, domains and domain suffixes to be able to do the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsed_url(url):\n",
    "    # extract subdomain, domain, and domain suffix from url\n",
    "    # if item == '', fill with '<empty>'\n",
    "    subdomain, domain, domain_suffix = ('<empty>' if extracted == '' else extracted for extracted in tldextract.extract(url))\n",
    "    return [subdomain, domain, domain_suffix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url(data):\n",
    "    # parsed url\n",
    "    extract_url_data = [parsed_url(url) for url in data['url']]\n",
    "    extract_url_data = pd.DataFrame(extract_url_data, columns=['subdomain', 'domain', 'domain_suffix'])\n",
    "    # concat extracted feature with main data\n",
    "    data = data.reset_index(drop=True)\n",
    "    data = pd.concat([data, extract_url_data], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = extract_url(train_data)\n",
    "val_data = extract_url(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(train_data, val_data, column):\n",
    "    plt.figure(figsize=(10, 17))\n",
    "    plt.subplot(411)\n",
    "    plt.title(f'Train data {column}')\n",
    "    plt.ylabel(column)\n",
    "    train_data[column].value_counts().head(10).plot.barh()\n",
    "    plt.subplot(412)\n",
    "    plt.title(f'Validation data {column}')\n",
    "    plt.ylabel(column)\n",
    "    val_data[column].value_counts().head(10).plot.barh()\n",
    "    plt.subplot(413)\n",
    "    plt.title(f'Train data {column} (groupped)')\n",
    "    plt.ylabel(f'(label, {column})')\n",
    "    train_data.groupby('label')[column].value_counts().head(10).plot.barh()\n",
    "    plt.subplot(414)\n",
    "    plt.title(f'Validation data {column} (groupped)')\n",
    "    plt.ylabel(f'(label, {column})')\n",
    "    val_data.groupby('label')[column].value_counts().head(10).plot.barh()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_data, val_data, 'subdomain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_data, val_data, 'domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_data, val_data, 'domain_suffix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot above there are interesting things to note, there are websites that have google and twitter domains with bad labels. It's time we do the filter to see data with google domains and Twitter with bad labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[(train_data['domain'] == 'google') & (train_data['label'] == 'bad')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[(train_data['domain'] == 'twitter') & (train_data['label'] == 'bad')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe some of there urls contain malware, who knows :) <br>\n",
    "The next step we need to do tokenization on the url so that it can be used as input to the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='', char_level=True, lower=False, oov_token=1)\n",
    "# fit only on training data\n",
    "tokenizer.fit_on_texts(train_data['url'])\n",
    "n_char = len(tokenizer.word_index.keys())\n",
    "print(f'N Char: {n_char}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(train_data['url'])\n",
    "val_seq = tokenizer.texts_to_sequences(val_data['url'])\n",
    "print('Before tokenization: ')\n",
    "print(train_data.iloc[0]['url'])\n",
    "print('\\nAfter tokenization: ')\n",
    "print(train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = np.array([len(i) for i in train_seq])\n",
    "sequence_length = np.percentile(sequence_length, 99).astype(int)\n",
    "print(f'Sequence length: {sequence_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each text length has a different length, therefore padding is needed to equalize each text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = pad_sequences(train_seq, padding='post', maxlen=sequence_length)\n",
    "val_seq = pad_sequences(val_seq, padding='post', maxlen=sequence_length)\n",
    "print('After padding: ')\n",
    "print(train_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to normalize so the value has a scale between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = train_seq / n_char\n",
    "val_seq = val_seq / n_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the tokenizer for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Features\n",
    "\n",
    "We will also encode subdomain, domain, suffix domains and label into numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label_index, data):\n",
    "    try:\n",
    "        return label_index[data]\n",
    "    except:\n",
    "        return label_index['<unknown>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_value = {}\n",
    "for feature in ['subdomain', 'domain', 'domain_suffix']:\n",
    "    # get unique value\n",
    "    label_index = {label: index for index, label in enumerate(train_data[feature].unique())}\n",
    "    # add unknown label in last index\n",
    "    label_index['<unknown>'] = list(label_index.values())[-1] + 1\n",
    "    # count unique value\n",
    "    unique_value[feature] = label_index['<unknown>']\n",
    "    # encode\n",
    "    train_data.loc[:, feature] = [encode_label(label_index, i) for i in train_data.loc[:, feature]]\n",
    "    val_data.loc[:, feature] = [encode_label(label_index, i) for i in val_data.loc[:, feature]]\n",
    "    # save label index\n",
    "    with open(f'{feature}.pkl', 'wb') as f:\n",
    "        pickle.dump(label_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [train_data, val_data]:\n",
    "    data.loc[:, 'label'] = [0 if i == 'good' else 1 for i in data.loc[:, 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique subdomain in Train data: {unique_value['subdomain']}\")\n",
    "print(f\"Unique domain in Train data: {unique_value['domain']}\")\n",
    "print(f\"Unique domain suffix in Train data: {unique_value['domain_suffix']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_block(x):\n",
    "    # 3 sequence conv layer\n",
    "    conv_3_layer = layers.Conv1D(64, 3, padding='same', activation='elu')(x)\n",
    "    # 5 sequence conv layer\n",
    "    conv_5_layer = layers.Conv1D(64, 5, padding='same', activation='elu')(x)\n",
    "    # concat conv layer\n",
    "    conv_layer = layers.concatenate([x, conv_3_layer, conv_5_layer])\n",
    "    # flatten\n",
    "    conv_layer = layers.Flatten()(conv_layer)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_block(unique_value, size):\n",
    "    input_layer = layers.Input(shape=(1,))\n",
    "    embedding_layer = layers.Embedding(unique_value, size, input_length=1)(input_layer)\n",
    "    return input_layer, embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_char, n_subdomain, n_domain, n_domain_suffix):\n",
    "    input_layer = []\n",
    "    # sequence input layer\n",
    "    sequence_input_layer = layers.Input(shape=(sequence_length,))\n",
    "    input_layer.append(sequence_input_layer)\n",
    "    # convolution block\n",
    "    char_embedding = layers.Embedding(n_char + 1, 32, input_length=sequence_length)(sequence_input_layer)\n",
    "    conv_layer = convolution_block(char_embedding)\n",
    "    # entity embedding\n",
    "    entity_embedding = []\n",
    "    for n in [n_subdomain, n_domain, n_domain_suffix]:\n",
    "        size = 4\n",
    "        input_l, embedding_l = embedding_block(n, size)\n",
    "        embedding_l = layers.Reshape(target_shape=(size,))(embedding_l)\n",
    "        input_layer.append(input_l)\n",
    "        entity_embedding.append(embedding_l)\n",
    "    # concat all layer\n",
    "    fc_layer = layers.concatenate([conv_layer, *entity_embedding])\n",
    "    fc_layer = layers.Dropout(rate=0.5)(fc_layer)\n",
    "    # dense layer\n",
    "    fc_layer = layers.Dense(128, activation='elu')(fc_layer)\n",
    "    fc_layer = layers.Dropout(rate=0.2)(fc_layer)\n",
    "    # output layer\n",
    "    output_layer = layers.Dense(1, activation='sigmoid')(fc_layer)\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[metrics.Precision(), metrics.Recall()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset session\n",
    "backend.clear_session()\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "# create model\n",
    "model = create_model(sequence_length, n_char, unique_value['subdomain'], unique_value['domain'], unique_value['domain_suffix'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png')\n",
    "model_image = mpimg.imread('model.png')\n",
    "plt.figure(figsize=(75, 75))\n",
    "plt.imshow(model_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above has 4 encoded inputs: \n",
    "1. the first input came from URL that has been done tokenization and padding. \n",
    "2. subdomains, \n",
    "3. domains, and \n",
    "4. suffix domains\n",
    " \n",
    "URL input will pass through embedding layer and convolution layer while other input will pass embedding layer. Then the results from each input will be concatenated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [train_seq, train_data['subdomain'], train_data['domain'], train_data['domain_suffix']]\n",
    "train_y = train_data['label']\n",
    "val_x = [val_seq, val_data['subdomain'], val_data['domain'], val_data['domain_suffix']]\n",
    "val_y = val_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = [EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True, mode='max')]\n",
    "history = model.fit(train_x, train_y, batch_size=64, epochs=25, verbose=1, validation_data=[val_x, val_y], shuffle=True, callbacks=early_stopping)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "for index, key in enumerate(['loss', 'precision', 'recall']):\n",
    "    plt.subplot(1, 3, index+1)\n",
    "    plt.plot(history.history[key], label=key)\n",
    "    plt.plot(history.history[f'val_{key}'], label=f'val {key}')\n",
    "    plt.legend()\n",
    "    plt.title(f'{key} vs val {key}')\n",
    "    plt.ylabel(f'{key}')\n",
    "    plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(val_x)\n",
    "val_pred = np.where(val_pred[:, 0] >= 0.5, 1, 0)\n",
    "print(f'Validation Data:\\n{val_data.label.value_counts()}')\n",
    "print(f'\\n\\nConfusion Matrix:\\n{confusion_matrix(val_y, val_pred)}')\n",
    "print(f'\\n\\nClassification Report:\\n{classification_report(val_y, val_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What is the precision of this mode?\n",
    "2. What is the recall of this mode?\n",
    "3. What is the relationship between high precision value and the classified malicious websites? \n",
    "4. What is the relationship between low precision value and the classified malicious websites? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
